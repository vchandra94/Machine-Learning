{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1)  What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "**Answer 1)**\n",
        "\n",
        "Information Gain is the core metric used by decision tree algorithms (like ID3) to figure out the best feature to split the data on at each step of building the tree.\n",
        "\n",
        "Think of it as a measure of how much clarity or reduction in uncertainty you get by splitting your data based on a particular feature. The goal is to always choose the split that provides the highest information gain.\n",
        "\n",
        "To understand Information Gain, you first need to understand Entropy.\n",
        "\n",
        "1. What is Entropy?\n",
        "\n",
        "    In simple terms, Entropy is a measure of impurity, disorder, or uncertainty in a dataset.\n",
        "\n",
        "    High Entropy (Max = 1): The dataset is perfectly impure. For a classification task, this means the classes are mixed randomly (e.g., a set with 50% \"Yes\" and 50% \"No\" has the highest possible entropy). It's very hard to make a decision.\n",
        "\n",
        "    Low Entropy (Min = 0): The dataset is perfectly pure. This means all samples in the set belong to the same class (e.g., 100% \"Yes\"). There is no uncertainty.\n",
        "\n",
        "\n",
        "    Example: Imagine you have two baskets of fruit:\n",
        "\n",
        "    i. Basket A: Contains 10 apples. It is pure. Its entropy is 0.\n",
        "\n",
        "\n",
        "    ii. Basket B: Contains 5 apples and 5 oranges. It is impure. Its entropy is 1\n",
        "\n",
        "2. **How is Information Gain Used in Decision Trees?**\n",
        "\n",
        "    The decision tree algorithm uses Information Gain to build the tree from the top down. At each \"node\" (a point where a decision is made), it does the following:\n",
        "\n",
        "    i) Calculate Parent Entropy: It first calculates the entropy of the current dataset (the \"parent node\") before any split.\n",
        "\n",
        "    ii) Calculate Child Entropies: For every possible feature (e.g., \"Outlook,\" \"Temperature,\" \"Humidity\"), it calculates what the entropy would be after splitting the data on that feature.\n",
        "    This involves:a. Splitting the data into subsets (the \"child nodes\").11 For example, splitting on \"Outlook\" creates three subsets: \"Sunny,\" \"Overcast,\" and \"Rainy.\"12b. Calculating the entropy for each of these child nodes.13c. Calculating the weighted average entropy of all the child nodes.(Subsets with more data points get a higher weight).\n",
        "\n",
        "    iii) Calculate Information Gain: It then finds the Information Gain for that feature using this formula:$$\\text{Information Gain} = \\text{Entropy}(\\text{Parent}) - \\text{Weighted Average Entropy}(\\text{Children})$$\n",
        "  \n",
        "    iv) Select the Best Feature: The algorithm repeats this for all features.15 The feature that results in the highest Information Gain is chosen as the splitting feature for that node.\n",
        "\n",
        "This process is repeated at each new node until the nodes are \"pure\" (entropy is 0) or another stopping condition is met."
      ],
      "metadata": {
        "id": "nwuGT23sIV6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2) What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "**Answer 2)**\n",
        "\n",
        "Both **Gini Impurity** and **Entropy** are measures of how mixed or impure a dataset is at a node in a decision tree — they help the algorithm decide where to split the data.\n",
        "\n",
        "Here’s the difference explained in simple terms:\n",
        "\n",
        "**Entropy** comes from information theory. It measures the amount of “uncertainty” or “disorder” in a dataset. If all the samples in a node belong to the same class, the entropy is zero — there’s no uncertainty. But if the samples are evenly split between classes, the entropy is high, meaning the node is very impure. Entropy uses logarithms to measure this uncertainty, and it tells us how much “information” is gained when we make a split (this is called *Information Gain*).\n",
        "\n",
        "**Gini Impurity**, on the other hand, measures how often a randomly chosen sample from the node would be incorrectly labeled if it were randomly assigned a label according to the class distribution. Like entropy, Gini is also zero when the node is pure, but it increases as the classes become more mixed.\n",
        "\n",
        "In practice, both give similar results, but Gini is a bit simpler and faster to compute because it doesn’t involve logarithms. Entropy is more theoretical, connecting to information theory, while Gini is more practical and often used as the default in algorithms like CART.\n",
        "\n",
        "In short:\n",
        "\n",
        "1. **Entropy** measures uncertainty using information theory.\n",
        "2. **Gini Impurity** measures the probability of misclassification.\n",
        "3. Both quantify impurity, but Gini is faster and usually preferred for efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "bhImN-iIL0QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3) What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "**Answer 3)**\n",
        "\n",
        "Pre-pruning is a technique used in decision trees to stop the tree from growing during the training process, before it becomes fully developed and overly complex.\n",
        "\n",
        "Think of it as setting \"early stopping rules\" for the tree's growth.\n",
        "\n",
        "The primary goal of pre-pruning is to prevent overfitting. A decision tree that grows without limits will try to perfectly classify every single sample in the training data, capturing not just the underlying patterns but also the noise. This \"perfect\" tree will then perform poorly on new, unseen data.\n",
        "\n",
        "\n",
        "Pre-pruning prevents this by halting the creation of new branches (or \"nodes\") if they don't meet certain criteria.\n",
        "\n",
        "How Pre-Pruning Works\n",
        "\n",
        "Pre-pruning works by setting hyperparameters that act as thresholds. While the tree is being built, it checks these rules at every potential split. If a rule is met, the tree stops splitting at that node, and it becomes a \"leaf node\" (a final decision).\n",
        "\n",
        "\n",
        "Common pre-pruning techniques (and their hyperparameters) include:\n",
        "\n",
        "1. Maximum Depth (max_depth): This is the most common technique. You set a limit on how many levels the tree can have. For example, if you set max_depth=3, the tree will stop growing after its third level of decisions, regardless of whether the nodes are \"pure\" or not.\n",
        "\n",
        "\n",
        "2. Minimum Samples per Split (min_samples_split): This rule specifies the minimum number of data points a node must have before it's even allowed to be split. If a node has fewer samples than this threshold, it will not be split and will become a leaf.\n",
        "\n",
        "\n",
        "3. Minimum Samples per Leaf (min_samples_leaf): This rule dictates that a split is only allowed if it results in both new child nodes having at least this many samples. This prevents the tree from creating tiny, highly specific leaves that are likely just fitting to noise.\n",
        "\n",
        "4. Minimum Impurity Decrease / Information Gain Threshold: You can set a threshold for how much \"purity\" (like Gini Impurity or Information Gain) a new split must achieve. If a potential split doesn't reduce the impurity by at least this amount, the tree won't bother making that split.\n",
        "\n"
      ],
      "metadata": {
        "id": "6oUd5i6lNT74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 4) Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Answer 4)\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (Iris dataset for example)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n",
        "# Optionally, check model accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy on Test Data: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m7UM9pOO7Qz",
        "outputId": "edf6b5a3-04d2-42d9-e624-5ec680201e4c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy on Test Data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5) What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "**Question 5)**\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used primarily for classification tasks, though it can also handle regression (in the form of Support Vector Regression). The main idea of SVM is to find the best boundary (hyperplane) that separates data points of different classes.\n",
        "\n",
        "Here’s a detailed explanation:\n",
        "\n",
        "1. Basic Concept\n",
        "\n",
        "    Suppose you have data points belonging to two classes (e.g., cats vs. dogs).\n",
        "\n",
        "    SVM tries to find a hyperplane that separates these two classes.\n",
        "\n",
        "    i) In 2D, this hyperplane is a line.\n",
        "\n",
        "    ii) In 3D, it’s a plane.\n",
        "\n",
        "    In higher dimensions, it’s a “hyperplane.”\n",
        "\n",
        "    The goal is to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class. These nearest points are called support vectors.\n",
        "\n",
        "2. Support Vectors\n",
        "\n",
        "    i) Support vectors are the data points that are closest to the hyperplane.\n",
        "\n",
        "    ii) They are crucial because the position of the hyperplane depends entirely on them.\n",
        "\n",
        "    iii) Other points that are farther away do not affect the hyperplane.\n",
        "\n",
        "3. Linear vs. Non-linear SVM\n",
        "\n",
        "    i) Linear SVM:\n",
        "    Works when data is linearly separable (can be separated by a straight line or hyperplane).\n",
        "\n",
        "    ii) Non-linear SVM:\n",
        "    For data that is not linearly separable, SVM uses a kernel trick to map the data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "4. Kernels\n",
        "\n",
        "    Kernels help SVM handle non-linear data. Common kernels include:\n",
        "\n",
        "    i) Linear kernel – for linearly separable data.\n",
        "\n",
        "    ii) Polynomial kernel – maps data into polynomial features.\n",
        "\n",
        "    iii) RBF (Radial Basis Function) kernel / Gaussian kernel – popular for complex boundaries.\n",
        "\n",
        "    iv) Sigmoid kernel – similar to neural network activation.\n",
        "\n",
        "5. Advantages of SVM\n",
        "\n",
        "    i) Works well in high-dimensional spaces.\n",
        "\n",
        "    ii) Effective even when number of features > number of samples.\n",
        "\n",
        "    iii) Uses support vectors, making it memory efficient.\n",
        "\n",
        "6. Limitations\n",
        "\n",
        "    i) Choosing the right kernel and hyperparameters can be tricky.\n",
        "\n",
        "    ii) Not ideal for very large datasets (training can be slow).\n",
        "\n",
        "    iii) Sensitive to noisy data and overlapping classes."
      ],
      "metadata": {
        "id": "LSm9OlcAW-1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6) What is the Kernel Trick in SVM?**\n",
        "\n",
        "**Answer 6)**\n",
        "\n",
        "The Kernel Trick is a powerful mathematical technique that allows Support Vector Machines (SVMs) to solve complex, non-linear classification problems.\n",
        "\n",
        "At its core, the trick allows the SVM to operate in a very high-dimensional (even infinite-dimensional) feature space without ever having to compute the coordinates of the data in that space. This avoids an enormous computational cost.\n",
        "\n",
        "Here’s a step-by-step breakdown:\n",
        "\n",
        "1. The Problem: Non-Linear Data\n",
        "\n",
        "A standard SVM works by finding the best linear separator (a hyperplane, which is just a line in 2D or a flat plane in 3D) that divides two classes.\n",
        "\n",
        "This works perfectly if the data is linearly separable.\n",
        "\n",
        "But what about data like this?\n",
        "\n",
        "You cannot draw a single straight line to separate the blue dots from the red dots. This data is non-linearly separable.\n",
        "\n",
        "2. The Solution: Map to a Higher Dimension\n",
        "\n",
        "The main idea to solve this is to transform the data into a higher dimension where it does become linearly separable.\n",
        "\n",
        "Imagine the \"donut\" data from above. It's in 2D (features $x_1, x_2$). Let's create a new, 3D space by applying a transformation function $\\phi$ (phi).\n",
        "\n",
        "Let's define a new feature, $z$, such that $z = x_1^2 + x_2^2$. Our new space is now 3D, with coordinates $(x_1, x_2, z)$.\n",
        "\n",
        "If we plot the data in this new 3D space, the red \"inner\" dots (which have small $x_1$ and $x_2$ values) will have a low $z$ value. The blue \"outer\" dots (which have large $x_1$ and $x_2$ values) will have a high $z$ value.\n",
        "\n",
        "In this new 3D space, the data is now perfectly separable by a simple 2D plane! The SVM can easily find this plane. When we project this plane back down to our original 2D space, it becomes the circular boundary we needed.\n",
        "\n",
        "3. The New Problem: The Cost of Transformation\n",
        "\n",
        "This transformation approach is brilliant, but it has a massive computational problem, known as the Curse of Dimensionality.\n",
        "\n",
        "The SVM algorithm relies heavily on one specific calculation: the dot product of data points (e.g., $\\vec{x_i} \\cdot \\vec{x_j}$).\n",
        "\n",
        "If we transform all our data points $\\vec{x}$ into the high-dimensional space $\\phi(\\vec{x})$, we would then have to compute the dot product in that new, very high-dimensional space: $\\phi(\\vec{x_i}) \\cdot \\phi(\\vec{x_j})$.\n",
        "\n",
        "This is computationally infeasible for two reasons:\n",
        "\n",
        "   i) Too Slow: Calculating $\\phi(\\vec{x})$ for every point can be extremely complex.\n",
        "\n",
        "  ii) Too Big: The new dimension can be so large (even infinitely large) that we can't store the $\\phi(\\vec{x})$ vectors.\n",
        "\n",
        "4. The \"Trick\": The Kernel Function\n",
        "\n",
        "This is where the magic happens. The Kernel Trick is based on the discovery that we don't need to know the coordinates $\\phi(\\vec{x})$ at all. We only need the result of their dot product: $\\phi(\\vec{x_i}) \\cdot \\phi(\\vec{x_j})$.\n",
        "\n",
        "A kernel function, written as $K(\\vec{x_i}, \\vec{x_j})$, is a special, computationally cheap function that takes the original low-dimensional vectors $\\vec{x_i}$ and $\\vec{x_j}$ as input and directly computes the dot product of their transformed, high-dimensional versions.\n",
        "\n",
        "In short, a kernel function lets us do this:\n",
        "\n",
        "$K(\\vec{x_i}, \\vec{x_j}) = \\phi(\\vec{x_i}) \\cdot \\phi(\\vec{x_j})$\n",
        "\n",
        "  a) The Left Side (What We Do): $K(\\vec{x_i}, \\vec{x_j})$. A simple, fast calculation using the original, low-dimensional data.\n",
        "\n",
        "  b) The Right Side (What We Implicitly Get): $\\phi(\\vec{x_i}) \\cdot \\phi(\\vec{x_j})$. The result of the dot product in the complex, high-dimensional space.\n",
        "\n",
        "The SVM algorithm can be rewritten to only use the kernel function $K(\\vec{x_i}, \\vec{x_j})$ instead of the standard dot product $\\vec{x_i} \\cdot \\vec{x_j}$.\n",
        "\n",
        "Summary: Why it's a \"Trick\"\n",
        "\n",
        "The Kernel Trick allows us to get all the separating power of a high-dimensional space without ever paying the computational price of transforming the data into that space.\n",
        "\n",
        "We just plug in a non-linear kernel function, and the SVM algorithm \"magically\" finds a complex, non-linear boundary in the original feature space.\n"
      ],
      "metadata": {
        "id": "7qPfWDnPPlsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 7 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Answer 7)\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create two SVM classifiers with different kernels\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"SVM Classifier Comparison on Wine Dataset\")\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy:    {acc_rbf:.4f}\")\n",
        "\n",
        "# Check which performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\n✅ Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\n✅ RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\n⚖️ Both kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8cbA5GSQulg",
        "outputId": "d84423c9-a753-4b06-b9aa-f116574f3806"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Comparison on Wine Dataset\n",
            "---------------------------------------\n",
            "Linear Kernel Accuracy: 0.9815\n",
            "RBF Kernel Accuracy:    0.7593\n",
            "\n",
            "✅ Linear kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8) What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "**Answer 8)**\n",
        "\n",
        "The Naïve Bayes classifier is a simple yet powerful and fast probabilistic machine learning algorithm. It is used for classification tasks, such as:\n",
        "\n",
        "i) Spam filtering: Classifying an email as \"Spam\" or \"Not Spam\".\n",
        "\n",
        "ii) Text classification: Determining the topic of an article (e.g., \"Sports,\" \"Technology,\" or \"Politics\").\n",
        "\n",
        "iii) Medical diagnosis: Predicting whether a patient has a certain disease based on their symptoms.\n",
        "\n",
        "The algorithm is based on Bayes' Theorem, which is a fundamental concept in probability.\n",
        "\n",
        "The \"Bayes\" Part: How It Works\n",
        "\n",
        "Bayes' Theorem provides a way to update our beliefs about something given new evidence. In the context of classification, the formula looks like this:\n",
        "\n",
        "$$P(\\text{Class} | \\text{Features}) = \\frac{P(\\text{Features} | \\text{Class}) \\cdot P(\\text{Class})}{P(\\text{Features})}$$\n",
        "\n",
        "Let's break this down with a spam filter example:\n",
        "\n",
        "1. $P(\\text{Class} | \\text{Features})$ (Posterior Probability): This is what we want to find. \"What is the probability the email is 'Spam', given that it contains the words 'Viagra' and 'free'?\"\n",
        "\n",
        "2. $P(\\text{Features} | \\text{Class})$ (Likelihood): \"How likely are the words 'Viagra' and 'free' to appear given that an email is 'Spam'?\" The algorithm learns this from the training data.\n",
        "\n",
        "3. $P(\\text{Class})$ (Prior Probability): \"What is the overall probability of any email being 'Spam'?\" (e.g., 20% of all emails are spam).\n",
        "\n",
        "4. $P(\\text{Features})$ (Evidence): \"What is the overall probability of seeing the words 'Viagra' and 'free' in any email?\" (We can often ignore this part, as it's the same for all classes).\n",
        "\n",
        "To make a prediction, the classifier calculates the posterior probability for every class (e.g., 'Spam' and 'Not Spam'). The class that results in the highest probability is the winner.\n",
        "\n",
        "**Why is it Called \"Naïve\"?**\n",
        "\n",
        "This is the most important part of its name. The algorithm is called \"Naïve\" because it makes a strong, simplifying assumption about the data that is almost always false in the real world.\n",
        "\n",
        "The \"Naïve\" Assumption: All features are independent of each other, given the class.\n",
        "\n",
        "In simple terms, the classifier naïvely believes that the presence (or absence) of one feature has absolutely no effect on the presence (or absence) of any other feature.\n",
        "\n",
        "Example of the \"Naïve\" Assumption\n",
        "\n",
        "Let's stick with our spam filter. The features are the words in the email.\n",
        "\n",
        "  a) Features: \"Viagra\", \"free\", \"congratulations\", \"lottery\"\n",
        "\n",
        "  b) Class: \"Spam\"\n",
        "\n",
        "Reality: In the real world, these words are highly correlated. An email containing \"Viagra\" is much more likely to also contain \"free\" and \"lottery\". The presence of one word gives us a strong hint about the others.\n",
        "\n",
        "The Naïve Bayes Classifier's View: The classifier assumes these words are completely unrelated. It calculates the probability of \"Viagra\" appearing in spam, the probability of \"free\" in spam, and the probability of \"lottery\" in spam, and then multiplies them together as if they were independent coin flips.\n",
        "\n",
        "It thinks that knowing \"Viagra\" is in the email gives it no information about whether \"free\" is also there. This is clearly an incorrect, or \"naïve,\" way to view the world."
      ],
      "metadata": {
        "id": "u94LcKO_Rvt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9) Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.**\n",
        "\n",
        "**Answer 9)**\n",
        "\n",
        "The main difference between Gaussian, Multinomial, and Bernoulli Naïve Bayes lies in the type of data they are designed to handle and the statistical distribution they assume for the features.\n",
        "\n",
        "All three are \"Naïve\" because they share the same core assumption: that all features are independent of one another, given the class.2 Where they differ is in how they model the probability of those features.\n",
        "\n",
        "Here’s a breakdown of each.\n",
        "\n",
        "1. Gaussian Naïve Bayes (GNB)\n",
        "\n",
        "    What it's for: Continuous numerical data.\n",
        "    \n",
        "    Core Idea: It assumes that the values for each feature are \"normally distributed\" (i.e., follow a Gaussian distribution, or bell curve) within each class.\n",
        "    \n",
        "    How it works: To calculate the probability of a feature value, the model first calculates the mean (4$\\mu$) and variance (5$\\sigma^2$) of that feature for each class from the training data.6 It then uses the Gaussian probability density function (PDF) to find the likelihood of a new data point.\n",
        "    \n",
        "    Simple Example: Predicting if a plant is Species A or Species B based on its petal_length and sepal_width in centimeters. The model would calculate the mean and variance of petal_length for all Species A plants, and separately for all Species B plants. It does the same for sepal_width.\n",
        "  \n",
        "2. Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "    What it's for: Discrete data, specifically \"counts\" or \"frequencies.\"\n",
        "    \n",
        "    Core Idea: It's designed for features that represent the count of an event occurring (e.g., how many times a word appears in a document).\n",
        "    \n",
        "    How it works: It calculates the probability of a feature (like a specific word) based on its average frequency within each class.\n",
        "    \n",
        "    Simple Example: Text classification (like spam filtering).\n",
        "    \n",
        "    Features: The count of each word in the vocabulary (e.g., word_count('free'), word_count('viagra'), word_count('meeting')).\n",
        "    \n",
        "    Data: A document might be represented as: {'free': 3, 'viagra': 1, 'meeting': 0, ...}.\n",
        "    \n",
        "    Model: It learns that the word \"free\" appears, on average, 5 times in \"Spam\" emails but only 0.1 times in \"Not Spam\" emails.\n",
        "    \n",
        "3. Bernoulli Naïve Bayes (BNB\n",
        "\n",
        "    What it's for: Binary/Boolean data (Yes/No, 1/0, True/False).\n",
        "    \n",
        "    Core Idea: It's used when features are binary variables, indicating the presence or absence of a feature.\n",
        "    \n",
        "    How it works: Instead of counting how many times a feature appears, it only cares if it appears at all.\n",
        "    \n",
        "    Simple Example: Also used for text classification, but with a different approach.\n",
        "    \n",
        "    Features: A binary value for each word in the vocabulary (e.g., contains('free'), contains('viagra'), contains('meeting')).\n",
        "    Data: The same document would be represented as: {'free': 1, 'viagra': 1, 'meeting': 0, ...}. The 1 for 'free' just means \"this word is present,\" ignoring that it appeared 3 times.\n",
        "    \n",
        "    Model: It learns the probability that the word \"free\" is present in a \"Spam\" email (e.g., 70% of spam emails contain 'free') vs. a \"Not Spam\" email (e.g., 5% of non-spam emails contain 'free')."
      ],
      "metadata": {
        "id": "-4LOSsnoTn4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 10) Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "Answer 10)\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train (fit) the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtXDWtjNWF62",
        "outputId": "875e93b9-2264-4932-97b1-9104738e7436"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: 97.37%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    }
  ]
}